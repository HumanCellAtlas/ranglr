---
title: "Validate a spreadsheet"
author: "Marion Shadbolt"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{validate-a-spreadsheet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Goals

This document aims to display some of the functionality of the `ranglr` R pacakge. The package aims to provide functions and tools to help primary wranglers wrangle their data and secondary wranglers review a dataset.

```{r setup}
library(ranglr)
library(magrittr)
```

# General Functionality

## Load a spreadsheet

This function loads a spreadsheet into R as a list of tibble objects. The list is named with the names of the tabs of the spreadsheet. This function discards the supplementary files tab if it is empty as well as the `Schemas` tab if it is present. It discards the top rows and retains the programmatic name as the names of the columns within each tab. The user just needs to provide the path to the spreadsheet they want to load.

```{r load_spreadsheet}
metadata_spreadsheet <- load_spreadsheet("../../../../Google Drive File Stream/My Drive/Brokering/PROJECTS - FINISHED/GSE95459 - GSE114374/hca-metadata-spreadsheet-GSE95459-GSE114374-colon.xlsx")
names(metadata_spreadsheet)
```

## Summarise a spreadsheet

I think it would be good to see an overall summary of the spreadsheet. This function is still under development so any ideas for additional summary stats are most welcome. Current functionality:
* number of expected bundles
* number of accessions

```{r summarise_spreadsheet}
summary <- summarise_spreadsheet(metadata_spreadsheet)
```

## Build a dataframe with list of all column names found in each sheet

Returns a tibble with two columns, `sheet_name` and `col_name`.

```{r get_col_names}
get_col_names(metadata_spreadsheet)
```


## Get a list of all filenames

This is useful if you want to compare the files present in a spreadsheet to those present in the S3 bucket for a project.

```{r get_file_names}
files <- get_file_names(metadata_spreadsheet)
files[1:5]
```

## Build a linking data frame

Creates a dataframe that shows where there is a linking field between two tabs in a spreadsheet.

```{r linking_df}
linking_df <- build_linking_df(metadata_spreadsheet)
linking_df
```

## Validate link levels

For each entry in the `linking_field` column of the `linking_df` I need to check that each tab has consistent naming and the same number of levels are present in each.

Ok which linking fields didn't match up

```{r non_matched_linking}
linking_df <- validate_link_levels(linking_df)
linking_df %>%
  dplyr::filter(! all_2_in_1 | ! all_1_in_2)
```

In this case all are okay.

## Secondary wrangler review process

### Reconstruct experiment

#### Graph diagram

Currently thinking of how to implement this.

#### Confirm files uploaded to s3

If you get the warning "Expected 4 pieces. Missing pieces filled with `NA` in 1 rows." this can be safely ignored.

```{r list_s3_files}
file_list_df <- list_s3_files(s3_url = "s3://org-humancellatlas-upload-staging/aaaaaaaa-bbbb-cccc-dddd-acf331bf0e8f/", 
                           user = "mshadbolt")

```

`check_uploaded_files` ensures all and only the files in the spreadsheet are in the s3 bucket and prints the results of the test to console.

```{r verify_files}
check_uploaded_files(files, file_list_df$file_name)
```

Example of using the `get_col_names` function if you don't remember which tab a certain field is in.

```{r get_col_names_example}
get_col_names(tibble_list = metadata_spreadsheet) %>%
  dplyr::filter(stringr::str_detect(col_name, "end_bias"))
```

Example of how to look at certain columns within a spreadsheet. You could decide you want to see all the ontology fields within the `Library preparation protocol` sheet

```{r}
metadata_spreadsheet$`Library preparation protocol` %>% 
  dplyr::select(dplyr::contains("ontology"))

```


#### check ontologies

Trying to work on better functionality but for now will just get a list of ontologies that we can do basic checks for vailidity

```{r}
ontology_col_names <- get_col_names(metadata_spreadsheet) %>%
  dplyr::filter(stringr::str_detect(col_name, "ontology|text")) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(not_field = get_not_field_name(col_name),
                field_name = get_field_name(col_name)) %>%
  dplyr::select(sheet_name, not_field, field_name) %>%
  dplyr::ungroup() %>%
  dplyr::group_nest(sheet_name, not_field, .key="field_names")

ontology_col_names

```


```{r}
ontologies <- purrr::map2(.x = ontology_col_names$sheet_name,
            .y=ontology_col_names$col_name,
            pull_field_levels, tibble_list=metadata_spreadsheet) %>%
  unlist()

sort(unique(ontologies))
```

```{r}
metadata_spreadsheet[[ontology_col_names$sheet_name[1]]] %>%
  dplyr::select(dplyr::one_of(ontology_col_names$col_name))
```


```{r}
test <- purrr::map2(.x = ontology_col_names$sheet_name,
                    .y = ontology_col_names$col_name,
                    get_rows, tibble_list = metadata_spreadsheet)

```



```{r}
split_tibble <- get_col_names(metadata_spreadsheet) %>%
  dplyr::filter(stringr::str_detect(col_name, "ontology|text")) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(col_type = get_field_name(col_name),
                not_field = get_not_field_name(col_name)) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(not_field) %>%
  dplyr::group_split()


split_tibble[[2]]$levels <-
purrr::map2(.x = split_tibble[[2]]$sheet_name,
            .y = split_tibble[[2]]$col_name,
            .f = pull_field_levels, tibble_list = metadata_spreadsheet)

tidyr::unnest(split_tibble[[2]])

expand_rows(split_tibble[[2]])

expanded_tibble_list <- purrr::map(split_tibble, expand_rows)
bound_tibble_list <- do.call(dplyr::bind_rows, expanded_tibble_list) 

ontology_table <- bound_tibble_list %>%
  dplyr::filter(col_type == "ontology") %>%
  dplyr::select(-col_type) %>%
  dplyr::rename(ontology = levels) %>%
  dplyr::group_by(not_field) %>%
  dplyr::mutate(rank = dplyr::row_number()) %>%
  dplyr::left_join(bound_tibble_list %>%
                     dplyr::filter(col_type == "ontology_label") %>%
                     dplyr::select(-col_type, -col_name) %>%
                     dplyr::rename(ontology_label = levels) %>%
                     dplyr::group_by(not_field) %>%
                     dplyr::mutate(rank = dplyr::row_number())) %>%
  dplyr::left_join(bound_tibble_list %>%
                     dplyr::filter(col_type == "text") %>%
                     dplyr::select(-col_type, -col_name) %>%
                     dplyr::rename(ontology_text = levels))
  
```





```{r}
get_ontologies(metadata_spreadsheet)
```



```{r}
named_sheet_names <- ontology_field_names$sheet_name
names(named_sheet_names) <- ontology_field_names$col_name
purrr::map2(.x = named_sheet_names,
            .y = ontology_field_names$col_name,
            .f = pull_field_levels, tibble_list = metadata_spreadsheet) %>%
  tibble::as_tibble()

    dplyr::rowwise() %>%
    dplyr::mutate(unique_entries = list(levels(factor(metadata_spreadsheet[[sheet_name]][[col_name]][which(!is.na(metadata_spreadsheet[[sheet_name]][[col_name]]))]))))
    dplyr::mutate(col_type = get_field_name(col_name),
                  not_field = get_not_field_name(col_name)) %>%
    dplyr::select(-col_name) %>%
    tidyr::spread(col_type, unique_entries)
```


```{r ontologies_per_tab}
ontologies_per_tab <- get_ontologies(metadata_spreadsheet) 
ontologies_per_tab[5,] %>%
  dplyr::rowwise() %>%
  tidyr::separate_rows(ontology, sep = "\\|\\|") %>%
  tidyr::separate_rows(ontology_label, sep = "\\|\\|")
  
```

```{r}

```


```{r vector_of_all_ontologies}
all_ontologies <- unname(unlist(sapply(ontologies_per_tab$ontology, split_field_list)))
all_ontologies[1:5]
```


Dani recommended using the ols api in order to get the most up to date versions of the ontologies.

```{r all_ontologies_in_HCA_ontology}


```

```{r all_ontologies_from_correct_category}

```

```{r}
ontologies
```


### Testing out ols api

```{r}
test_api <- httr::GET("https://ontology.staging.data.humancellatlas.org/api/ontologies/hcao/terms?obo_id=UO:0000008")
test_api$status_code

test_api <- httr::GET("https://ontology.staging.data.humancellatlas.org/api/ontologies/uberon/terms?obo_id=UBERON:0000465")

test_api <- query_hca_ontology_term("EFO:0008995", "hcao")
test_api <- query_hca_ontology_term("EFO:0008995", "efo")
test_api <- query_hca_ontology_term("HANCESTRO:0005", "hancestro")
test_api <- query_hca_ontology_term("MONDO:0000001", "mondo")
test_api <- query_hca_ontology_term("UBERON:0000465", "hcao")
test_api <- query_hca_ontology_term("UO:0000008", "efo")
test_api <- query_hca_ontology_term("NCBITaxon:9606", "efo")
test_api <- query_hca_ontology_term("HsapDv:0000000", "hcao")
HsapDv:0000000


response <- httr::content(test_api, as = "text", encoding = "UTF-8")
json_list <- jsonlite::fromJSON(response, flatten = TRUE)

json_list$'_embedded'%>% 
  unlist() %>% 
  t() %>% 
  tibble::as_tibble() %>%
  dplyr::select(terms.obo_id, terms.label, terms.is_obsolete, terms.description)
```

Need a lookup table to know which ontologies are allowable for each ontology field

```{r}
all_fields_spreadsheet <- load_spreadsheet("~/Desktop/git_repos/ranglr/input/all_fields_spreadsheet.xlsx")
get_col_names(all_fields_spreadsheet) %>%
  dplyr::filter(stringr::str_detect(col_name, "ontology$")) %>%
  readr::write_csv("~/Desktop/git_repos/ranglr/input/ontology_fields.csv")
```


```{r}
ontology_lookup <- lapply(ontologies, query_hca_ontology_term)
```


<!-- Leaving this here because I might be able to use simliar code and don't want
to forget about these functions-->
<!-- ```{r} -->
<!-- purrr::map2_dfr(.x = metadata_spreadsheet, -->
<!--                 .y = metadata_spreadsheet, -->
<!--                 .f = ~get_linking_fields(.x, .y)) %>% View() -->

<!-- purrr::map2_dfr(.x = metadata_spreadsheet, -->
<!--                 .y = names(metadata_spreadsheet), -->
<!--                 .f = colnames) -->
<!-- ``` -->


